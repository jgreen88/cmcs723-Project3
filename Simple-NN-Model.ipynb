{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats.stats import pearsonr\n",
    "import nltk\n",
    "import pprint, pickle\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.normalization import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras import backend as K\n",
    "from keras.regularizers import *\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "\n",
    "# requires monolingual word aligner\n",
    "old_path = os.getcwd()\n",
    "os.chdir('/home/sri/Desktop/monolingual-word-aligner')\n",
    "from aligner import *\n",
    "os.chdir(old_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files = ['MSRpar.txt', 'MSRvid.txt', 'SMTeuroparl.txt']\n",
    "test_files = ['answer-answer.txt', 'headlines.txt', 'plagiarism.txt', \\\n",
    "              'postediting.txt', 'question-question.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSRpar.txt 750\n",
      "MSRvid.txt 750\n",
      "SMTeuroparl.txt 734\n"
     ]
    }
   ],
   "source": [
    "raw_train_data = {}\n",
    "\n",
    "for t in train_files:\n",
    "    pkl_file = open(\"preprocessed_feats/\" + t + \".features_pkl\", 'rb')\n",
    "\n",
    "    data = pickle.load(pkl_file)\n",
    "    raw_train_data[t] = data\n",
    "    print t, len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LEN = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# process train data for each individual file\n",
    "train_data = {}\n",
    "for t in raw_train_data:\n",
    "    tups = raw_train_data[t]\n",
    "    \n",
    "    file_X_train = []\n",
    "    file_y_train = []\n",
    "    \n",
    "    #mlen = 0\n",
    "    \n",
    "    i = 0\n",
    "    for (s1, s2, es1, es2, scr) in tups:\n",
    "        avg_es1 = np.mean(es1, axis = 1)\n",
    "        avg_es2 = np.mean(es2, axis = 1)\n",
    "        feats = np.abs(avg_es1 - avg_es2)\n",
    "        \n",
    "        cs = cosine_similarity(avg_es1.reshape(1, -1), avg_es2.reshape(1, -1))\n",
    "        feats = np.append(feats, cs.flatten())\n",
    "        \n",
    "        file_X_train.append(feats)\n",
    "        file_y_train.append(scr / 5.0)\n",
    "        \n",
    "        print \"      \\r\", str(i),\n",
    "        i += 1\n",
    "    \n",
    "    file_X_train = np.array(file_X_train)\n",
    "    file_y_train = np.array(file_y_train)\n",
    "       \n",
    "    train_data[t] = (file_X_train, file_y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 301)\n",
      "(734, 301)\n",
      "(750, 301)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "val_data = {}\n",
    "\n",
    "# create train-val split\n",
    "for t in train_data:\n",
    "    file_X_train_all, file_y_train_all = train_data[t]\n",
    "    print file_X_train_all.shape\n",
    "    \n",
    "    file_X_train, file_X_val, file_y_train, file_y_val = \\\n",
    "        train_test_split(file_X_train_all, file_y_train_all, test_size = 0.1, random_state = 42)\n",
    "\n",
    "    train_data[t] = (file_X_train, file_y_train)\n",
    "    val_data[t] = (file_X_val, file_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 301)\n",
      "(660, 301)\n",
      "(675, 301)\n"
     ]
    }
   ],
   "source": [
    "# final training thingy\n",
    "X_train = None\n",
    "y_train = None\n",
    "\n",
    "for t in train_data:\n",
    "    file_X, file_y = train_data[t]\n",
    "    print file_X.shape\n",
    "    \n",
    "    if (X_train is None):\n",
    "        X_train = file_X\n",
    "    else:\n",
    "        X_train = np.append(X_train, file_X, axis = 0)\n",
    "        \n",
    "    if (y_train is None):\n",
    "        y_train = file_y\n",
    "    else:\n",
    "        y_train = np.append(y_train, file_y, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2010, 301)\n",
      "(2010,)\n",
      "(675, 301)\n",
      "(675,)\n",
      "(675, 301)\n",
      "(675,)\n",
      "(675, 301)\n",
      "(675,)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "\n",
    "for v in val_data:\n",
    "    val_X, val_y = train_data[t]\n",
    "    print val_X.shape\n",
    "    print val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_38 (Dense)                 (None, 128)           38656       dense_input_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "prelu_13 (PReLU)                 (None, 128)           128         dense_38[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 128)           0           prelu_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_39 (Dense)                 (None, 64)            8256        dropout_27[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "prelu_14 (PReLU)                 (None, 64)            64          dense_39[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)             (None, 64)            0           prelu_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_40 (Dense)                 (None, 32)            2080        dropout_28[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "prelu_15 (PReLU)                 (None, 32)            32          dense_40[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)             (None, 32)            0           prelu_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_41 (Dense)                 (None, 16)            528         dropout_29[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "prelu_16 (PReLU)                 (None, 16)            16          dense_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)             (None, 16)            0           prelu_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_42 (Dense)                 (None, 1)             17          dropout_30[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 49777\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim = X_train.shape[1]))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = Adam(),\n",
    "      loss = 'mean_absolute_error')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.2103     \n",
      "Epoch 2/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1458     \n",
      "Epoch 3/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1346     \n",
      "Epoch 4/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1290     \n",
      "Epoch 5/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1213     \n",
      "Epoch 6/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1164     \n",
      "Epoch 7/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1128     \n",
      "Epoch 8/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1097     \n",
      "Epoch 9/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1084     \n",
      "Epoch 10/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1041     \n",
      "Epoch 11/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.1007     \n",
      "Epoch 12/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0998     \n",
      "Epoch 13/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0947     \n",
      "Epoch 14/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0927     \n",
      "Epoch 15/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0944     \n",
      "Epoch 16/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0881     \n",
      "Epoch 17/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0847     \n",
      "Epoch 18/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0856     \n",
      "Epoch 19/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0796     \n",
      "Epoch 20/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0802     \n",
      "Epoch 21/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0754     \n",
      "Epoch 22/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0761     \n",
      "Epoch 23/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0732     \n",
      "Epoch 24/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0714     \n",
      "Epoch 25/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0697     \n",
      "Epoch 26/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0711     \n",
      "Epoch 27/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0662     \n",
      "Epoch 28/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0666     \n",
      "Epoch 29/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0630     \n",
      "Epoch 30/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0617     \n",
      "Epoch 31/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0623     \n",
      "Epoch 32/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0606     \n",
      "Epoch 33/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0611     \n",
      "Epoch 34/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0559     \n",
      "Epoch 35/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0555     \n",
      "Epoch 36/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0553     \n",
      "Epoch 37/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0554     \n",
      "Epoch 38/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0552     \n",
      "Epoch 39/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0546     \n",
      "Epoch 40/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0522     \n",
      "Epoch 41/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0533     \n",
      "Epoch 42/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0510     \n",
      "Epoch 43/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0503     \n",
      "Epoch 44/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0507     \n",
      "Epoch 45/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0491     \n",
      "Epoch 46/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0491     \n",
      "Epoch 47/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0472     \n",
      "Epoch 48/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0488     \n",
      "Epoch 49/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0470     \n",
      "Epoch 50/50\n",
      "2010/2010 [==============================] - 0s - loss: 0.0482     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce8570bb50>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, nb_epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSRpar.txt 0.337958906444\n",
      "SMTeuroparl.txt 0.692522671614\n",
      "MSRvid.txt 0.853355443603\n"
     ]
    }
   ],
   "source": [
    "for v in val_data:\n",
    "    val_X, val_y = val_data[v]\n",
    "    \n",
    "    pred_y = model.predict(val_X).flatten()   \n",
    "    \n",
    "    corr, _ = pearsonr(val_y, pred_y)\n",
    "    print v, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    }
   ],
   "source": [
    "for t in test_files:\n",
    "    pkl_file = open(\"preprocessed_feats/\" + t + \".features_pkl\", 'rb')\n",
    "\n",
    "    tups = pickle.load(pkl_file)\n",
    "    \n",
    "    file_X_train = []\n",
    "    file_y_train = []\n",
    "    \n",
    "    i = 0\n",
    "    for (s1, s2, es1, es2, scr) in tups:\n",
    "        avg_es1 = np.mean(es1, axis = 1)\n",
    "        avg_es2 = np.mean(es2, axis = 1)\n",
    "        feats = np.abs(avg_es1 - avg_es2)\n",
    "        \n",
    "        cs = cosine_similarity(avg_es1.reshape(1, -1), avg_es2.reshape(1, -1))\n",
    "        feats = np.append(feats, cs.flatten())\n",
    "        \n",
    "        file_X_train.append(feats)\n",
    "        file_y_train.append(scr / 5.0)\n",
    "        \n",
    "        print \"      \\r\", str(i),\n",
    "        i += 1\n",
    "    \n",
    "    file_X_train = np.array(file_X_train)\n",
    "    pred_y = model.predict(file_X_train).flatten()\n",
    "    np.savetxt(t[:-4] + \".predictions\", pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in test_data:\n",
    "    test_X, test_y = test_data[v]\n",
    "    \n",
    "    pred_y = model.predict(val_X).flatten()   \n",
    "    \n",
    "    corr, _ = pearsonr(val_y, pred_y)\n",
    "    print v, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
